{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmieraufgabe II  ( 6+6+12+6 = 30 Punkte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ska(x, y):\n",
    "    \n",
    "    return x.T.dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teilaufgabe a:  Gradientenverfahren mit Armijo-Schrittweite (6 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie das Gradientenverfahren mit Armijo-Linesearch:\n",
    "\n",
    "Als Input sollen übergeben werden: \n",
    "\n",
    "* eine Funktion $f$ sowie eine weitere Funktion $\\operatorname{grad} f$, die den Gradienten von $f$ berechnet,  \n",
    "* ein Startwert $x_0$,\n",
    "* eine Toleranz $\\text{tol} > 0$ sowie die maximale Anzahl $\\text{it}_\\text{max}$ an Gradientenschritten, \n",
    "* die zwei Parameter $\\beta$, $\\gamma$ für die Schrittweitenbestimmung nach Armijo sowie die maximale Anzahl $\\text{armijoit}_\\text{max}$ der dafür ausgeführten Iterationen.\n",
    "\n",
    "Output soll eine Iterierte $x_{k^*}$ sein, die $\\lVert \\nabla f(x_{k^*}) \\rVert < \\text{tol}$ erfüllt. Zusätzlich sollen Listen mit den Funktionswerten $(f(x_k))_k$ ($\\texttt{fvals}$) bzw. den Iterierten $(x_k)_k$ ($\\texttt{xvals}$) ausgegeben werden. \n",
    "\n",
    "Im Falle des Versagens des Algorithmus bzw. von Teilen des Algorithmus sollen entsprechende Fehlermeldungen ausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ersetze 'd'\n",
    "\n",
    "\n",
    "def armijo_schritt(f, gradf, x, beta, gamma, armijoitmax):\n",
    "    \n",
    "    sigma = 1.\n",
    "    while f(x + sigma*d) - f(x) > sigma*gamma*ska(gradf(x), d):\n",
    "        sigma *= beta\n",
    "    \n",
    "    return sigma\n",
    "\n",
    "\n",
    "def gradientdescent_armijo(f, gradf, x0, tol, beta, gamma, itmax, armijoitmax):\n",
    "    \n",
    "    x = x0\n",
    "    xvals = [x]\n",
    "    fvals = [f(x)]\n",
    "    \n",
    "    it = 0\n",
    "    while np.linalg.norm(gradf(x)) > tol and it < itmax:\n",
    "        \n",
    "        sigma = armijo_schritt(f, gradf, x, beta, gamma, armijoitmax)\n",
    "        x = x + sigma*d\n",
    "        \n",
    "        xvals.append(x)\n",
    "        fvals.append(f(x))\n",
    "        \n",
    "        it += 1\n",
    "        \n",
    "    if np.linalg.norm(gradf(x)) > tol:\n",
    "        print(\"precision error\")\n",
    "    \n",
    "    return x, xvals, fvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen Sie Ihre Implementierung an folgenden Beispielen (mit $x^*$ sind jeweils die bekannten Minima angegeben): \n",
    "\n",
    "* $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}, (x_1, x_2) \\mapsto 100(x_2 -x_1^2)^2 + (1-x_1)^2$, $x^* = (1,1)^T$.  \n",
    "* $g: \\mathbb{R}^{100} \\rightarrow \\mathbb{R}, x \\mapsto \\frac{1}{2} x^T A x - b^T x$, mit $b = Ax^*$, $x^* = (1,1,...,1)^T$. $A$ ist die $100 \\times 100$-Matrix mit $4$en auf der Hauptdiagonale, $-1$ auf den beiden Nebendiagonalen und sonst Nulleinträgen.  \n",
    "* $h: \\mathbb{R} \\rightarrow \\mathbb{R}$, $x \\mapsto \\operatorname{exp}(-1/x^2)$, $x^* = 0$.  \n",
    "\n",
    "Plotten Sie dazu sowohl die Verläufe von $f(x_k)-f(x^*)$ als auch von $\\lVert x_k - x^* \\rVert_2$ (analog für $g$,$h$) für den Startwerten $x_0 = 0$ ($f$ und $g$) bzw. $x_0 = 1$ (für $h$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "def gradf(x):\n",
    "    dx1 = 400 * (x[0]**3- x[0]*x[1]) + 2 * (x[0] - 1)\n",
    "    dx2 = 200 * (x[1] - x[0]**2)\n",
    "    return np.array([dx1, dx2])\n",
    "\n",
    "def g(x):\n",
    "    return 1/2 * (np.sum(2*x*x) + x[0]**2 + x[99]**2) - (np.sum(2*x) + x[0] + x[99])\n",
    "    # 1/2 * (3*(x[0]**2) + 2*(x[1]**2) + ... + 2*(x[98]**2) + 3*(x[99]**2)) - (3*x[0] + 2*x[1] + ... + 2*x[98] + 3*x[99])\n",
    "def gradg(x):\n",
    "    dx = 2*(x-1)\n",
    "    dx[0] += x[0]-1\n",
    "    dx[99] += x[99]-1\n",
    "    # (3*(x[0]-1), 2*(x[1]-1), ..., 2*(x[98]-1), 3*(x[99]-1))\n",
    "    return dx\n",
    "\n",
    "def h(x):\n",
    "    return np.exp(-1/x[0]**2)\n",
    "def gradh(x):\n",
    "    dx1 = 2/(x[0]**3) * np.exp(1-1/(x[0]**2))\n",
    "    return np.array([dx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ff5411623a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradientdescent_armijo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marmijoitmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(\"g:\", gradientdescent_armijo(g, gradg, np.zeros(100), tol, c, beta, itmax, armijoitmax))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(\"h:\", gradientdescent_armijo(h, gradh, np.zeros(1), tol, c, beta, itmax, armijoitmax))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2f6e158894ed>\u001b[0m in \u001b[0;36mgradientdescent_armijo\u001b[0;34m(f, gradf, x0, tol, beta, gamma, itmax, armijoitmax)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mitmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marmijo_schritt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marmijoitmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2f6e158894ed>\u001b[0m in \u001b[0;36marmijo_schritt\u001b[0;34m(f, gradf, x, beta, gamma, armijoitmax)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mska\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "tol = 1.\n",
    "itmax = 100\n",
    "armijoitmax = 100\n",
    "beta = .5\n",
    "gamma = .01\n",
    "\n",
    "print(\"f:\", gradientdescent_armijo(f, gradf, np.zeros(2), tol, beta, gamma, itmax, armijoitmax))\n",
    "#print(\"g:\", gradientdescent_armijo(g, gradg, np.zeros(100), tol, c, beta, itmax, armijoitmax))\n",
    "#print(\"h:\", gradientdescent_armijo(h, gradh, np.zeros(1), tol, c, beta, itmax, armijoitmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teilaufgabe b: Gradientenverfahren mit konstanter Schrittweite (6 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie analog zu Teilaufgabe a) den Gradientenabstieg mit konstanter Schrittweite $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO pruefe, ob funktioniert\n",
    "\n",
    "\n",
    "def gradientdescent_const(f, gradf, x0, tol, alpha, itmax):\n",
    "    \n",
    "    x = x0\n",
    "    xvals = [x]\n",
    "    fvals = [f(x)]\n",
    "    \n",
    "    it = 0\n",
    "    while np.linalg.norm(gradf(x)) > tol and it < itmax:\n",
    "        \n",
    "        x = x - alpha\n",
    "        \n",
    "        xvals.append(x)\n",
    "        fvals.append(f(x))\n",
    "        \n",
    "        it += 1\n",
    "        \n",
    "    if np.linalg.norm(gradf(x)) > tol:\n",
    "        print(\"precision error\")\n",
    "    \n",
    "    return x, xvals, fvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleichen Sie das Konvergenzverhalten des Gradientenverfahrens mit konstanter Schrittweite mit Ihren Ergebnissen aus \n",
    "Teilaufgabe a). Verwenden Sie dazu dieselben drei Funktionen $f$, $g$ und $h$, die gleichen Startwerte und testen Sie Schrittweiten $\\alpha \\in \\{ 0.5, 0.3, 0.1, 0.01, 0.005, 0.003, 0.002 \\}$. Interpretieren Sie Ihr Ergebnis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1.\n",
    "itmax = 100\n",
    "alpha = 0.5\n",
    "\n",
    "x, xvals, fvals = gradientdescent_const(f, gradf, np.zeros(2), tol, alpha, itmax)\n",
    "print(\"f:\", \"\\n\\tx =\", x, \"\\n\\txvals = \", xvals, \"\\n\\tfvals = \", fvals)\n",
    "\n",
    "x, xvals, fvals = gradientdescent_const(g, gradg, np.zeros(100), tol, alpha, itmax)\n",
    "print(\"g:\", \"\\n\\tx =\", x, \"\\n\\txvals = \", xvals, \"\\n\\tfvals = \", fvals)\n",
    "\n",
    "x, xvals, fvals = gradientdescent_const(h, gradh, np.zeros(1), tol, alpha, itmax)\n",
    "print(\"h:\", \"\\n\\tx =\", x, \"\\n\\txvals = \", xvals, \"\\n\\tfvals = \", fvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teilaufgabe c: Gradientenverfahren vs. CG-Verfahren  (12 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie für eine positiv definite Matrix $A$, eine rechte Seite $b$ und einen Startwert $x_0$ den Gradientenabstieg mit exakter Schrittweite für die Lösung der linearen Gleichung $Ax = b$. Neben $A$ und $b$ sollen als Input eine Toleranz $\\text{tol}$ für die Norm des Gradienten sowie eine maximale Anzahl $\\text{it}_\\text{max}$ von Gradientenschritten vorgegeben werden. \n",
    "\n",
    "Implementieren Sie analog dazu das CG-Verfahren für $Ax = b$ und vergleichen Sie die Konvergenzgeschwindigkeit des Gradientenverfahrens mit der des CG-Verfahrens anhand von $A$, $b$ aus Teilaufgabe a): Erstellen Sie dazu eine Graphik, in der Sie die Konvergenzgeschwindikeit aller vier Verfahren (Gradientenabstieg mit Armijo-Schrittweite, konstanter Schrittweite, optimaler Schrittweite und CG) gegenüberstellen.\n",
    "\n",
    "Interpretieren Sie Ihr Ergebnis. Gehen Sie dabei auf die Konvergenzordnung des Gradientenverfahrens mit optimaler Schrittweite bzw. des CG-Verfahrens ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientdescent_quadratic(A, b, x0, tol, itmax):\n",
    "    \n",
    "    ## to be filled in ##\n",
    "    \n",
    "    return x, xvals\n",
    "\n",
    "def mycg(A, b, x0, itmax, tol):\n",
    "    \n",
    "    xvals = []\n",
    "    i = 0\n",
    "    ri = A.dot(x0)-b\n",
    "    beta = 0\n",
    "    xi = np.copy(x0)\n",
    "    \n",
    "    di = np.copy(-ri)#im ersten schritt di = -ri\n",
    "    ri_norm_new = np.linalg.norm(ri)\n",
    "    xvals.append(xi)\n",
    "    \n",
    "    \n",
    "    for _ in range(A.shape[0]):\n",
    "        \n",
    "        if (ri_norm_new < tol or i>itmax):\n",
    "            print(i)\n",
    "            break\n",
    "        \n",
    "        t = A.dot(di)\n",
    "        \n",
    "        alpha = (ri_norm_new*ri_norm_new)/ska(t, di)\n",
    "        \n",
    "        xi = xi +alpha*di\n",
    "        ri = ri +alpha*t\n",
    "        xvals.append(xi)\n",
    "        \n",
    "        ri_norm_old = ri_norm_new\n",
    "\n",
    "        ri_norm_new = np.linalg.norm(ri)\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "        beta = (ri_norm_new*ri_norm_new)/(ri_norm_old*ri_norm_old)\n",
    "        \n",
    "        di = -ri +beta*di\n",
    "    \n",
    "    \n",
    "    return xi, xvals\n",
    "\n",
    "\n",
    "A = np.array([[4,1],[1,3]])\n",
    "b = np.array([1,2])\n",
    "x0 = np.array([2,1])\n",
    "res, xvals = mycg(A, b, x0, 10, 0.001)\n",
    "print(res)\n",
    "print(np.matmul(A,res) - b)\n",
    "#print(np.matmul(A,res))\n",
    "print(xvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vergleich der 4 Verfahren: ## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sei nun $C = \\begin{pmatrix} c & -1 \\\\-1 & c \\end{pmatrix}$ mit $c > 1$ and $d = (0,0)^T$. Lassen Sie die Iterierten des Gradientenverfahrens mit optimaler Schrittweite bzw. des CG-Verfahrens (für die lineare Gleichung $Cx = d$) in einen Höhenlinienplot der Funktion $\\phi(x) = \\frac{1}{2} x^T C x - d^T x$ einzeichnen. Startwert soll für beide Verfahren $x_0 = (c,1)^T$ sein.\n",
    "\n",
    "Variieren Sie $c$ und interpretieren Sie Ihr Ergebnis. Achten Sie dabei darauf, dass der Plot einen sinnvollen Ausschnitt der \"Höhenlandschaft\" von $f$ zeigt, sodass das Verhalten der Iteration erkennbar wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(C, x1, x2):\n",
    "    x = np.array([x1,x2])\n",
    "    #print(x)\n",
    "    return 0.5*np.dot(x.T,np.dot(C,x))\n",
    "def h(C, X, Y):\n",
    "    res = np.zeros_like(X)\n",
    "    it = np.nditer(res, flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        it[0][...] = phi(C, X[it.multi_index[0]][it.multi_index[1]], Y[it.multi_index[0]][it.multi_index[1]])\n",
    "        it.iternext()\n",
    "    return res\n",
    "\n",
    "c = 1.2\n",
    "\n",
    "C = np.array([[c,-1],[-1,c]])\n",
    "x0 = np.array([c,1])\n",
    "b = np.array([0,0])\n",
    "res, xvals= mycg(A,b,x0,10, 0.001)\n",
    "print(xvals)\n",
    "\n",
    "x = np.linspace(-2,2,50)\n",
    "y = np.linspace(-2,2,50)\n",
    "\n",
    "X, Y =np.meshgrid(x,y)\n",
    "\n",
    "Z = h(C, X, Y)\n",
    "#plt.contour(X, Y, Z, colors=\"black\")\n",
    "plt.contour(X, Y, Z, 30, cmap='RdGy')\n",
    "plt.plot([x[0] for x in xvals],[x[1] for x in xvals])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teilaufgabe d: Gauss-Newton Verfahren (6 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie das Gauss-Newton-Verfahren von Übungsblatt 4, Aufgabe 2. Für die Lösung der entsprechenden Least-Squares Probleme können Sie die unten angegebene Funktion $\\texttt{myleastsquares}$ verwenden.\n",
    "\n",
    "Input der Funktion $\\texttt{gaussnewton}$ soll sein: \n",
    "* Funktion $f$ sowie eine Funktion $\\textrm{d} f$, die die Jacobi-Matrix von $f$ auswertet   \n",
    "* Vektor $y$ \n",
    "* Starwert $x_0$ und Toleranz $\\text{tol}$, derart, dass die Iteration abbricht, wenn $\\lVert \\nabla r(x) \\rVert < \\text{tol}$ erfüllt ist oder eine Maximalzahl $\\text{it}_\\text{max}$ von Iterationen erreicht ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myleastsquares(A,b):\n",
    "    Q,R = np.linalg.qr(A)\n",
    "    rhs = (np.transpose(Q)).dot(b)\n",
    "    xmin = np.linalg.solve(R,rhs)\n",
    "    return xmin \n",
    "\n",
    "def gaussnewton(f,df,y,x0,tol,itmax): \n",
    "    \n",
    "    ## to be filled in ##\n",
    "    \n",
    "    return x, xvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen Sie Ihre Implementierung anhand des Beispiels aus Aufgabe 2c von Übungsblatt 4. Erstellen Sie einen Plot, aus dem hervorgeht, welche Konvergenzordnung beobachtet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
